{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "ANLPwTF_AnyaMit.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOKiteGFEuM3NZKSXl0ItHm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnyaMit/ANLPwTF/blob/main/ANLPwTF_AnyaMit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrTcBT4BzcEN"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import io\n",
        "\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_topov9z1Fv"
      },
      "source": [
        "# Download the zip file\n",
        "\n",
        "path_to_zip = tf.keras.utils.get_file(\"smsspamcollection.zip\",origin=\"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\",extract=True)\n",
        "\n",
        "# Unzip the file int a folder\n",
        "\n",
        "!unzip $path_to_zip -d data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIC292mX0N4b"
      },
      "source": [
        "# Let's seee if we  read the data correctly\n",
        "\n",
        "lines = io.open('data/SMSSpamCollection').read().strip().split('\\n')\n",
        "lines[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3SCObUw0gHo"
      },
      "source": [
        "## PRE PROCESSING DATA SECTION"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmGneZHN0wFR"
      },
      "source": [
        "spam_dataset = []\n",
        "for line in lines:\n",
        "  label, text = line.split('\\t')\n",
        "  if label.strip() == 'spam':\n",
        "    spam_dataset.append((1, text.strip()))\n",
        "  else:\n",
        "    spam_dataset.append((0, text.strip()))\n",
        "print(spam_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TnZlO4311Qz6"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(spam_dataset, columns=['Spam','Message'])\n",
        "\n",
        "import re\n",
        "\n",
        "def message_length(x):\n",
        "  # returns totaal number  of characters\n",
        "  return len(x)\n",
        "\n",
        "def num_capitals(x):\n",
        "  _, count = re.subn(r'[A-Zz]', '', x) # only works in english\n",
        "  return count\n",
        "\n",
        "def num_punctuation(x):\n",
        "  _, count = re.subn(r'\\W', '', x) \n",
        "  return count\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zLWGC9p2uHD"
      },
      "source": [
        "df['Capitals'] = df['Message'].apply(num_capitals)\n",
        "df['Punctuation'] = df['Message'].apply(num_punctuation)\n",
        "df['Length'] = df['Message'].apply(message_length)\n",
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfWsLirM4Fs_"
      },
      "source": [
        "## Split into Test and Train\n",
        "\n",
        "train = df.sample(frac=0.8, random_state = 42)\n",
        "test = df.drop(train.index)\n",
        "\n",
        "x_train = train[['Length','Capitals','Punctuation']]\n",
        "y_train = train[['Spam']]\n",
        "\n",
        "x_test = test[['Length','Capitals','Punctuation']]\n",
        "y_test = test[['Spam']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvrfgacZ4j87"
      },
      "source": [
        "## Model Normalization\n",
        "\n",
        "# 1- Layer neural network model for evalutaion\n",
        "\n",
        "def make_model(input_dims = 3, num_units = 12):\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  # Add a densely - connected layer with 12 units to the model:\n",
        "  model.add(tf.keras.layers.Dense(num_units,\n",
        "                                  input_dim=input_dims,\n",
        "                                  activation='relu'))\n",
        "  \n",
        "  # Add a sigmoid layer with a binary output unit:\n",
        "  model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "  model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEqO5Axb5VaD"
      },
      "source": [
        "model = make_model()\n",
        "\n",
        "model.fit(x_train, y_train, epochs=10, batch_size=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ev7-VUwo8Yf1"
      },
      "source": [
        "model.evaluate(x_test,y_test)\n",
        "\n",
        "y_train_pred = model.predict_classes(x_train)\n",
        "\n",
        "# confusion matrix\n",
        "\n",
        "tf.math.confusion_matrix(tf.constant(y_train.Spam),\n",
        "                         y_train_pred)\n",
        "\n",
        "#  array([[3733,  134],\n",
        "#       [ 133,  459]], dtype=int32)>\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7lLpODV9C5F"
      },
      "source": [
        "sentence = 'Go until Jurong point, crazy.. Available onlly in bugis n great world'\n",
        "sentence.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whvRnk4l9zOr"
      },
      "source": [
        "!pip install stanza \n",
        "import stanza\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHkEu1nm-A4m"
      },
      "source": [
        "en = stanza.download('en')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYjt_Wbsf2zw"
      },
      "source": [
        "en = stanza.Pipeline(lang='en', processors='tokenize')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ojr1I6W_e8x"
      },
      "source": [
        "tokenized = en(sentence)\n",
        "len(tokenized.sentences)\n",
        "\n",
        "for snt in tokenized.sentences:\n",
        "  for word in snt.tokens:\n",
        "    print(word.text)\n",
        "  print(\"<End of Sentence>\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd3T9LJV_kMA"
      },
      "source": [
        "en = stanza.Pipeline(lang='en') \n",
        "print(en)\n",
        "\n",
        "def word_counts(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = sum([len(sentence.tokens) for sentence in doc.sentences])\n",
        "  return count\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkz1y0KKAK5q"
      },
      "source": [
        "## This did not work due to the error described in the book. No work around found for the Pytorch issue....\n",
        "\n",
        "#/usr/local/lib/python3.7/dist-packages/stanfordnlp/models/depparse/model.py:157: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/cuda/LegacyDefinitions.cpp:28.)\n",
        "# unlabeled_scores.masked_fill_(diag, -float('inf'))\n",
        "\n",
        "train['Words'] = train['Message'].apply(word_counts)\n",
        "test['Words'] = test['Message'].apply(word_counts)\n",
        "\n",
        "x_train = train[['Length', 'Punctuation', 'Capitals', 'Words']]\n",
        "y_train = train['Spam']\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals', 'Words']]\n",
        "y_test = test['Spam']\n",
        "\n",
        "model = make_model(input_dims=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSRwi10tBXWH"
      },
      "source": [
        "model.fit(x_train,  y_train, epochs=10, batch_size =10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-VWN9fiBh_J"
      },
      "source": [
        "train.loc[train.Spam == 1].describe()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1X9niz8eI-L3"
      },
      "source": [
        "!pip install stopwordsiso\n",
        "\n",
        "import stopwordsiso as stopwords\n",
        "\n",
        "stopwords.langs()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HJKmkxjJcI0"
      },
      "source": [
        "sorted(stopwords.stopwords('en'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmmSk806JhqX"
      },
      "source": [
        "en_sw = stopwords.stopwords('en')\n",
        "\n",
        "def word_counts(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  count = 0\n",
        "  for sentence in doc.sentences:\n",
        "    for token in sentence.tokens:\n",
        "      if token.text.lower() not in en_sw:\n",
        "        count += 1\n",
        "       # count = count.astype('float32')\n",
        "  return count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVrTEoU7J4Lm"
      },
      "source": [
        "## Modeling with stopwords removed\n",
        "\n",
        "train['Words'] = train['Message'].apply(word_counts)\n",
        "test['Words'] = test['Message'].apply(word_counts)\n",
        "\n",
        "x_train = train[['Length', 'Punctuation', 'Capitals', 'Words']]\n",
        "y_train = train['Spam']\n",
        "\n",
        "x_test = test[['Length', 'Punctuation', 'Capitals', 'Words']]\n",
        "y_test = test['Spam']\n",
        "\n",
        "model = make_model(input_dims=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUdYueXiK3UN"
      },
      "source": [
        "## POS Tagging\n",
        "\n",
        "en = stanza.Pipeline(lang='en')\n",
        "print(en)\n",
        "\n",
        "txt = \"Yo you around? A friend of mine's lookin.\"\n",
        "pos = en(txt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeTVeQCmT5an"
      },
      "source": [
        "def print_pos(doc):\n",
        "  text = \"\"\n",
        "  for sentence in doc.sentences:\n",
        "    for token in sentence.tokens:\n",
        "      text += token.words[0].text+\"/\"+ token.words[0].upos+ \" \"\n",
        "    text += \"\\n\"\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2N1PktCUXO4"
      },
      "source": [
        "print(print_pos(pos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI0L0nU7UV7E"
      },
      "source": [
        "en_sw = stopwords.stopwords('en')\n",
        "\n",
        "def word_counts_v3(x, pipeline=en):\n",
        "  doc = pipeline(x)\n",
        "  totals = 0.\n",
        "  count = 0.\n",
        "  non_word = 0.\n",
        "  for sentence in doc.sentences:\n",
        "    for token in sentence.tokens:\n",
        "      if token.text.lower() not in en_sw:\n",
        "        if token.words[0].upos not in ['PUNKT','SYM']:\n",
        "          count += 1.\n",
        "        else:\n",
        "          non_word += 1.\n",
        "  non_word = non_word / totals\n",
        "  return pd.Series([count, non_word], index = ['Words_NoPunct', 'Punct'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1PviyQePUgXh"
      },
      "source": [
        "## Skipping page 32-34 as there is an error -- submitted issue https://github.com/stanfordnlp/stanfordnlp/issues/8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uAM30oRuVA30"
      },
      "source": [
        "corpus = [\n",
        "          \"I like fruits. Fruits like bananas\",\n",
        "          \"I love bananas but eat an apple\",\n",
        "          \"An apple a day keeps the doctor away\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWKQ-5h0VM_U"
      },
      "source": [
        "!pip install sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyzfSvyeVPp0"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "vectorizer.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRYoXdAKVa1R"
      },
      "source": [
        "X.toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IISvrWZ6VdLx"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "cosine_similarity(X.toarray())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNgAKibSVi2T"
      },
      "source": [
        "query = vectorizer.transform([\"apple and bananas\"])\n",
        "\n",
        "cosine_similarity(X, query)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvSw1J9nVjoN"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "transformer = TfidfTransformer(smooth_idf=False)\n",
        "tfidf = transformer.fit_transform(X.toarray())\n",
        "\n",
        "pd.DataFrame(tfidf.toarray(),\n",
        "             columns = vectorizer.get_feature_names())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxLIekGUXS3C"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "tfidf = TfidfVectorizer(binary=True)\n",
        "\n",
        "X = tfidf.fit_transform(train['Message']).astype('float32')\n",
        "X_test = tfidf.transform(test['Message']).astype('float32')\n",
        "\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Ta4cBaZT_q"
      },
      "source": [
        "_, cols = X.shape\n",
        "\n",
        "model2 = make_model(cols) # to match td-idf dimensions\n",
        "\n",
        "y_train = train[['Spam']]\n",
        "y_test = test[['Spam']]\n",
        "\n",
        "model2.fit(X.toarray(), y_train, epochs = 10, batch_size = 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xC4r7OUZrQ2"
      },
      "source": [
        "model2.evaluate(X_test.toarray(), y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FwiRaUT2Z1YU"
      },
      "source": [
        "y_test_pred = model2.predict_classes(X_test.toarray())\n",
        "tf.math.confusion_matrix(tf.constant(y_test.Spam), y_test_pred)\n",
        "\n",
        "## Much better performance\n",
        "## array([[958,   2],\n",
        "## [ 17, 138]], dtype=int32)>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69R3EcW_aPEE"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALQ3kzPZarGo"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader as api\n",
        "model_w2v = api.load(\"word2vec-google-news-300\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jL-a63za2jn"
      },
      "source": [
        "model_w2v.most_similar(\"cookies\", topn=10)\n",
        "model_w2v.doesnt_match([\"USA\",\"Canada\",\"India\",\"Tokyo\"])\n",
        "\n",
        "king = model_w2v['king']\n",
        "woman = model_w2v['woman']\n",
        "man = model_w2v['man']\n",
        "\n",
        "queen = king - man + woman\n",
        "model_w2v.similar_by_vector(queen)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v16C9p5Rbkd2"
      },
      "source": [
        "#### Chapter 2 --- NLU ----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uK0Pi3Vbha7"
      },
      "source": [
        "!pip install tensorflow_datasets\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0eIA2S5eWXi"
      },
      "source": [
        "\", \".join(tfds.list_builders())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5WJmVPargDsB"
      },
      "source": [
        "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\", split=\"train\", with_info = True, as_supervised=True)\n",
        "imdb_test = tfds.load(name=\"imdb_reviews\",split=\"test\",as_supervised=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNU_9GnFgNPM"
      },
      "source": [
        "print(ds_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYKzGw37gl2k"
      },
      "source": [
        "for example, label in imdb_train.take(1):\n",
        "  print(example, '\\n', label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRuc_5EdhA_w"
      },
      "source": [
        "tokenizer = tfds.deprecated.text.Tokenizer() #Used this instead of what the book used --- why: https://github.com/tensorflow/tensorflow/issues/45217"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn4T5AIRhqUR"
      },
      "source": [
        "\n",
        "vocabulary_set = set()\n",
        "MAX_TOKENS = 0\n",
        "\n",
        "for example, label in imdb_train:\n",
        "  some_tokens = tokenizer.tokenize(example.numpy())\n",
        "  if MAX_TOKENS < len(some_tokens):\n",
        "    MAX_TOKENS = len(some_tokens)\n",
        "  vocabulary_set.update(some_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "skrf5cDAipdy"
      },
      "source": [
        "imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set, tokenizer=tokenizer)\n",
        "vocab_size = imdb_encoder.vocab_size\n",
        "print(vocab_size, MAX_TOKENS)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMDkTSmUi67-"
      },
      "source": [
        "for example, label in imdb_train.take(1):\n",
        "  print (example)\n",
        "  encoded = imdb_encoder.encode(example.numpy())\n",
        "  print(imdb_encoder.decode(encoded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqL2ornhjLID"
      },
      "source": [
        "imdb_encoder.save_to_file(\"reviews_vocab\")\n",
        "enc = tfds.deprecated.text.TokenTextEncoder.load_from_file(\"reviews_vocab\")\n",
        "enc.decode(enc.encode(\"Good case. Excellent value\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgdEabdhjTvK"
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "def encode_pad_transform(sample):\n",
        "  encoded = imdb_encoder.encode(sample.numpy())\n",
        "  pad = sequence.pad_sequences([encoded], padding = 'post', maxlen = 150)\n",
        "  return np.array(pad[0], dtype=np.int64)\n",
        "\n",
        "def encode_tf_fn(sample, label):\n",
        "  encoded = tf.py_function(encode_pad_transform, inp=[sample], Tout = (tf.int64))\n",
        "  encoded.set_shape([None])\n",
        "  label.set_shape([])\n",
        "  return encoded, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dGTLYoTkbUC"
      },
      "source": [
        "subset = imdb_train.take(10)\n",
        "tst = subset.map(encode_tf_fn)\n",
        "for review, label in tst.take(1):\n",
        "  print(review,label)\n",
        "  print(imdb_encoder.decode(review))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBLSjqJUkpCv"
      },
      "source": [
        "# Running on the entire set\n",
        "\n",
        "encoded_train = imdb_train.map(encode_tf_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "encoded_test = imdb_test.map(encode_tf_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "coS0YSdKlE9n"
      },
      "source": [
        "LSTM model with embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCr8NYpUk5fP"
      },
      "source": [
        "tf.keras.layers.LSTM(rnn_units) ## another function that doesn't work..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASWY1llvmPST"
      },
      "source": [
        "def build_model_lstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "          tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                    mask_zero = True,\n",
        "                                    batch_input_shape =[batch_size, None]),\n",
        "          tf.keras.layers.LSTM(rnn_units),\n",
        "          tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HdLQxKFnaNr"
      },
      "source": [
        "vocab_size = imdb_encoder.vocab_size\n",
        "\n",
        "# The embedding dimention\n",
        "embedding_dim = 64\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 64\n",
        "\n",
        "# batch size\n",
        "BATCH_SIZE = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dPrG2qWn408"
      },
      "source": [
        "model = build_model_lstm(\n",
        "    vocab_size = vocab_size,\n",
        "    embedding_dim = embedding_dim,\n",
        "    rnn_units = rnn_units,\n",
        "    batch_size = BATCH_SIZE)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhYr0qCtpn3X"
      },
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics = ['accuracy', 'Precision', 'Recall'])\n",
        "\n",
        "encoded_train_batched = encoded_train.batch(BATCH_SIZE)\n",
        "model.fit(encoded_train_batched, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xm8r53CcqCgY"
      },
      "source": [
        "model.evaluate(encoded_test.batch(BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tta3eehIqXI2"
      },
      "source": [
        "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "          tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                    mask_zero = True,\n",
        "                                    batch_input_shape =[batch_size, None]),\n",
        "          tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(rnn_units)),\n",
        "          tf.keras.layers.Dense(1,activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7PnZ2D2qxXu"
      },
      "source": [
        "bilstm = build_model_bilstm(\n",
        "    vocab_size = vocab_size,\n",
        "    embedding_dim = embedding_dim,\n",
        "    rnn_units = rnn_units,\n",
        "    batch_size = BATCH_SIZE)\n",
        "\n",
        "bilstm.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0x0UXZnNslTQ"
      },
      "source": [
        "bilstm.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics = ['accuracy', 'Precision', 'Recall'])\n",
        "\n",
        "encoded_train_batched = encoded_train.batch(BATCH_SIZE)\n",
        "\n",
        "bilstm.fit(encoded_train_batched, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX0GshnMusG7"
      },
      "source": [
        "Chapter 3: NER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtJED8TPvwRi"
      },
      "source": [
        "Chapter 4: Transfer Learning with BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8h2jRbZur5j"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "imdb_train, ds_info = tfds.load(name=\"imdb_reviews\",\n",
        "                                split=\"train\",\n",
        "                                with_info=True, as_supervised= True)\n",
        "\n",
        "imdb_test = tfds.load(name=\"imdb_reviews\", split=\"test\", as_supervised= True)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ub_1inJCzJi7"
      },
      "source": [
        "# Use default tokenizer settings\n",
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "\n",
        "vocabulary_set = set()\n",
        "MAX_TOKENS = 0\n",
        "\n",
        "for example, label in imdb_train:\n",
        "  some_tokens = tokenizer.tokenize(example.numpy())\n",
        "  if MAX_TOKENS < len(some_tokens):\n",
        "    MAX_TOKENS = len(some_tokens)\n",
        "  vocabulary_set.update(some_tokens)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO-3kErszx3k"
      },
      "source": [
        "imdb_encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set, lowercase = True, tokenizer=tokenizer)\n",
        "vocab_size = imdb_encoder.vocab_size\n",
        "print(vocab_size, MAX_TOKENS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu9y5QBw0DXd"
      },
      "source": [
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "def encode_pad_transform(sample):\n",
        "  encoded = imdb_encoder.encode(sample.numpy())\n",
        "  pad = sequence.pad_sequences([encoded], padding = 'post', maxlen = 150)\n",
        "  return np.array(pad[0], dtype=np.int64)\n",
        "\n",
        "def encode_tf_fn(sample, label):\n",
        "  encoded = tf.py_function(encode_pad_transform, inp=[sample], Tout = (tf.int64))\n",
        "  encoded.set_shape([None])\n",
        "  label.set_shape([])\n",
        "  return encoded, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPqnytVO0TkR"
      },
      "source": [
        "# Running on the entire set\n",
        "\n",
        "encoded_train = imdb_train.map(encode_tf_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "encoded_test = imdb_test.map(encode_tf_fn, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96YKCzXm0XN2"
      },
      "source": [
        "## Download pre-trained embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAWs30F40iiz"
      },
      "source": [
        "dict_w2v = {}\n",
        "with open('glove.6B.50d.txt',\"r\") as file:\n",
        "  for line in file:\n",
        "    tokens = line.split()\n",
        "    word = tokens[0]\n",
        "    vector = np.array(tokens[1:], dtype=np.float32)\n",
        "\n",
        "    if vector.shape[0] == 50:\n",
        "      dict_w2v[word] = vector\n",
        "    else:\n",
        "      print(\"There was an issue with \" + word)\n",
        "\n",
        "# Lets check the vocab size\n",
        "print(\"Dictionary Size:\", len(dict_w2v))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz8z-yHX1VO9"
      },
      "source": [
        "embedding_dim = 50\n",
        "embedding_matrix = np.zeros((imdb_encoder.vocab_size, embedding_dim))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4cXZAXC1l_D"
      },
      "source": [
        "unk_cnt = 0\n",
        "unk_set = set()\n",
        "for word in imdb_encoder.tokens:\n",
        "  embedding_vector = dict_w2v.get(word)\n",
        "\n",
        "  if embedding_vector is not None:\n",
        "    tkn_id = imdb_encoder.encode(word)[0]\n",
        "    embedding_matrix[tkn_id] = embedding_vector\n",
        "  else:\n",
        "    unk_cnt += 1\n",
        "    unk_set.add(word)\n",
        "\n",
        "# Print how many weren't found\n",
        "print(\"Total unknown words:\", unk_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTA78PMS2K_V"
      },
      "source": [
        "##################### FEATURE EXTRACTION MODEL ##########################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGJM82HS2VPL"
      },
      "source": [
        "# Length of vocab in chars\n",
        "vocab_size = imdb_encoder.vocab_size #len chars\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 64\n",
        "\n",
        "# batch size\n",
        "BATCH_SIZE = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmGRkM-s2jUv"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense\n",
        "\n",
        "def build_model_bilstm(vocab_size, embedding_dim, rnn_units, batch_size, train_emb=False):\n",
        "  model = tf.keras.Sequential([\n",
        "          Embedding(vocab_size, embedding_dim,\n",
        "                                    mask_zero = True,\n",
        "                                    weights = [embedding_matrix],\n",
        "                                    trainable = train_emb),\n",
        "  Bidirectional(LSTM(rnn_units, return_sequences=True,dropout=0.5)),\n",
        "  Bidirectional(LSTM(rnn_units,dropout=0.25)),\n",
        "  Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qO-cBaRj3hf9"
      },
      "source": [
        "model_fe = build_model_bilstm(\n",
        "    vocab_size = vocab_size,\n",
        "    embedding_dim = embedding_dim,\n",
        "    rnn_units = rnn_units,\n",
        "    batch_size = BATCH_SIZE)\n",
        "\n",
        "model_fe.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuAStQCn4DS2"
      },
      "source": [
        "model_fe.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics = ['accuracy', 'Precision', 'Recall'])\n",
        "\n",
        "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)\n",
        "\n",
        "model_fe.fit(encoded_train_batched, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6Jyqq414eoV"
      },
      "source": [
        "model_fe.evaluate(encoded_test.batch(BATCH_SIZE))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64_M-tJw4se2"
      },
      "source": [
        "##################### FINE TUNING MODEL ##########################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZzEy5_m4uRB"
      },
      "source": [
        "model_ft = build_model_bilstm(\n",
        "    vocab_size = vocab_size,\n",
        "    embedding_dim = embedding_dim,\n",
        "    rnn_units = rnn_units,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    train_emb = True)\n",
        "\n",
        "model_ft.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SipLjBkc42jJ"
      },
      "source": [
        "model_ft.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics = ['accuracy', 'Precision', 'Recall'])\n",
        "\n",
        "encoded_train_batched = encoded_train.batch(BATCH_SIZE).prefetch(100)\n",
        "\n",
        "model_ft.fit(encoded_train_batched, epochs=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llCCq0jt51l7"
      },
      "source": [
        "!pip install transformers==3.0.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfMvvpv7FPgZ"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "bert_name = 'bert-base-cased'\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_name,\n",
        "                                          add_special_tokens=True,\n",
        "                                          do_lower_case = False,\n",
        "                                          max_length=150,\n",
        "                                          pad_to_max_length=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_hCJO2rFiNs"
      },
      "source": [
        "tokenizer.encode_plus(\"Don't be lured\", add_special_tokens=True,\n",
        "                      max_length = 9,\n",
        "                      pad_to_max_length = True,\n",
        "                      return_attention_mask = True,\n",
        "                      return_token_type_ids=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt6CdqQBGXVn"
      },
      "source": [
        "tokenizer.encode_plus(\" Don't be\", \" lured\", add_special_tokens=True,\n",
        "                      max_length = 10,\n",
        "                      pad_to_max_length = True,\n",
        "                      return_attention_mask = True,\n",
        "                      return_token_type_ids=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPySHRnOGcFv"
      },
      "source": [
        "def bert_encoder(review):\n",
        "  txt = review.numpy().decode('utf-8')\n",
        "  encoded = tokenizer.encode_plus(txt, add_special_tokens=True,\n",
        "                                  max_length = 150,\n",
        "                                  pad_to_max_length = True,\n",
        "                                  return_attention_mask = True,\n",
        "                                  return_token_type_ids=True)\n",
        "  \n",
        "  return encoded['input_ids'], encoded['token_type_ids'], encoded['attention_mask']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B8dYNyEeHBoT"
      },
      "source": [
        "bert_train = [bert_encoder(r) for r, l in imdb_train]\n",
        "bert_lbl = [l for r, l in imbd_train]\n",
        "bert_train = np.array(bert_train)\n",
        "bert_lbl = tf.keras.utils.to_categorical(bert_lbl, num_classes=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lr3VEK11HfHG"
      },
      "source": [
        "# create training and validation splits \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_val, y_train, y_val = train_test_Split(bert_train,\n",
        "bert_lbl,\n",
        "test_size=0.2,\n",
        "random_state=42))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}